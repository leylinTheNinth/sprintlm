<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>SprintLM — a tiny GPT you can read</title>
  <meta name="description" content="SprintLM: a tiny GPT-style Transformer implemented from scratch and trained on TinyStories in ~1 hour. Clean code, reproducible runs, plots, and samples." />
  <meta property="og:title" content="SprintLM — a tiny GPT you can read" />
  <meta property="og:description" content="From-scratch GPT-style Transformer. 1×T4, ~1 hour, 23.0M tokens, val loss 2.713 → ppl 15.07." />
  <meta property="og:type" content="website" />
  <meta name="theme-color" content="#0ea5e9" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg: #0b1020;           /* deep blue-gray */
      --panel: #0f172a;        /* slate-900 */
      --muted: #94a3b8;        /* slate-400 */
      --text: #e2e8f0;         /* slate-200 */
      --accent: #22d3ee;       /* cyan-400 */
      --accent-2: #a78bfa;     /* violet-400 */
      --line: #1e293b;         /* slate-800 */
      --ring: rgba(34,211,238,.35);
    }
    * { box-sizing: border-box; }
    html, body { margin: 0; padding: 0; background: var(--bg); color: var(--text); font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    .container { max-width: 1120px; margin: 0 auto; padding: 24px; }
    header.hero { position: relative; padding: 72px 0 32px; }
    .grad {
      position: absolute; inset: -10% -10% auto -10%; height: 360px; filter: blur(64px); pointer-events: none; opacity: .65;
      background: radial-gradient(60% 60% at 20% 40%, rgba(34,211,238,.35), transparent 60%),
                  radial-gradient(50% 50% at 70% 20%, rgba(167,139,250,.35), transparent 60%);
    }
    .title { font-size: clamp(32px, 5vw, 54px); line-height: 1.05; letter-spacing: -0.02em; margin: 0 0 16px; }
    .subtitle { font-size: clamp(16px, 2.5vw, 20px); color: var(--muted); max-width: 820px; }
    .actions { display: flex; gap: 12px; margin-top: 24px; flex-wrap: wrap; }
    .btn { display: inline-flex; align-items: center; gap: 10px; padding: 10px 14px; background: linear-gradient(90deg, var(--accent), var(--accent-2)); color: #081226; border-radius: 12px; font-weight: 600; border: 1px solid rgba(255,255,255,.08); box-shadow: 0 10px 30px rgba(34,211,238,.15); }
    .btn.secondary { background: transparent; color: var(--text); border: 1px solid #2b3346; }
    .cards { display: grid; grid-template-columns: repeat(4, 1fr); gap: 16px; margin: 36px 0 8px; }
    .card { background: linear-gradient(180deg, rgba(255,255,255,.03), rgba(255,255,255,.02)); border: 1px solid #1f2942; border-radius: 16px; padding: 16px; }
    .k { color: var(--muted); font-size: 12px; text-transform: uppercase; letter-spacing: .12em; }
    .v { font-size: 22px; font-weight: 700; color: #e6f9ff; }
    .section { padding: 40px 0; border-top: 1px solid var(--line); }
    h2 { font-size: 22px; margin: 0 0 14px; letter-spacing: .01em; }
    p.lead { color: var(--muted); max-width: 880px; }
    .grid2 { display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; }
    figure { margin: 0; background: #0e1528; border: 1px solid #1f2942; border-radius: 16px; padding: 12px; }
    figure img { width: 100%; display: block; border-radius: 10px; }
    figure figcaption { color: var(--muted); font-size: 13px; margin-top: 8px; }
    .arch { background: #0e1528; border: 1px solid #1f2942; border-radius: 16px; padding: 16px; }
    .code { background: #0e1528; border: 1px solid #1f2942; border-radius: 12px; padding: 12px; overflow: auto; font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; font-size: 13px; }
    blockquote { margin: 0; padding: 12px 16px; background: #0e1528; border-left: 3px solid var(--accent); border-radius: 8px; color: #dbeafe; }
    details { background: #0e1528; border: 1px solid #1f2942; border-radius: 12px; padding: 10px 12px; }
    footer { color: var(--muted); font-size: 13px; padding: 32px 0 48px; }
    @media (max-width: 900px) { .cards { grid-template-columns: repeat(2, 1fr);} .grid2 { grid-template-columns: 1fr; } }
  </style>
</head>
<body>
  <header class="hero">
    <div class="grad"></div>
    <div class="container">
      <h1 class="title">SprintLM — a tiny GPT you can read end‑to‑end</h1>
      <p class="subtitle">From-scratch GPT‑style Transformer (RoPE, pre‑RMSNorm, SwiGLU) trained on TinyStories in ~1 hour on a single T4. Clean code, reproducible runs, plots, and sample generations.</p>
      <div class="actions">
        <a class="btn" href="https://github.com/leylinTheNinth/sprintlm.git" target="_blank" rel="noopener">View on GitHub</a>
        <a class="btn secondary" href="#reproduce">Reproduce</a>
      </div>
      <div class="cards">
        <div class="card"><div class="k">Validation</div><div class="v">loss 2.713 → ppl 15.07</div></div>
        <div class="card"><div class="k">Tokens</div><div class="v">23.04M (5k×24×192)</div></div>
        <div class="card"><div class="k">Throughput</div><div class="v">~7.5k tok/s</div></div>
        <div class="card"><div class="k">Model</div><div class="v">12L · d384 · 6H</div></div>
      </div>
    </div>
  </header>

  <section class="section">
    <div class="container">
      <h2>At a glance</h2>
      <p class="lead">Decoder‑only Transformer with Rotary Position Embeddings, pre‑RMSNorm, and SwiGLU FFN. Tokenization via GPT‑2 BPE (tiktoken), one <code>&lt;|endoftext|&gt;</code> per story. Training loop uses NumPy memmap tokens (uint16) for streaming throughput, Hydra configs for reproducibility, and per‑run artifacts (metrics.csv, checkpoints, PNG plots).</p>
    </div>
  </section>

  <section class="section">
    <div class="container">
      <h2>Training run</h2>
      <div class="grid2">
        <figure>
          <img src="assets/loss_train.png" alt="Train Loss" />
          <figcaption>Train loss over 5,000 steps (batch 24, context 192).</figcaption>
        </figure>
        <figure>
          <img src="assets/loss_val.png" alt="Val Loss" />
          <figcaption>Validation loss (TinyStories val set). Final: 2.713 → ppl 15.07.</figcaption>
        </figure>
        <figure>
          <img src="assets/tokps.png" alt="Tokens per second" />
          <figcaption>Throughput on a free Colab T4. Dips show eval/ckpt intervals.</figcaption>
        </figure>
        <div class="arch">
          <h3 style="margin-top:0">Architecture (simplified)</h3>
          <svg viewBox="0 0 820 240" width="100%" height="auto" role="img" aria-label="Architecture diagram">
            <defs>
              <linearGradient id="g1" x1="0" x2="1"><stop offset="0" stop-color="#22d3ee"/><stop offset="1" stop-color="#a78bfa"/></linearGradient>
            </defs>
            <rect x="10" y="40" width="110" height="60" rx="10" fill="#0b1224" stroke="#1f2942" />
            <text x="65" y="75" text-anchor="middle" fill="#e2e8f0" font-size="12">Token
Embedding</text>
            <rect x="140" y="20" width="240" height="100" rx="12" fill="#0b1224" stroke="#1f2942" />
            <text x="260" y="48" text-anchor="middle" fill="#94a3b8" font-size="12">×12</text>
            <text x="260" y="70" text-anchor="middle" fill="#e2e8f0" font-size="12">Transformer Block</text>
            <text x="260" y="88" text-anchor="middle" fill="#94a3b8" font-size="11">(RoPE · MHA · SwiGLU · RMSNorm)</text>
            <rect x="400" y="40" width="110" height="60" rx="10" fill="#0b1224" stroke="#1f2942" />
            <text x="455" y="75" text-anchor="middle" fill="#e2e8f0" font-size="12">RMSNorm</text>
            <rect x="530" y="40" width="120" height="60" rx="10" fill="#0b1224" stroke="#1f2942" />
            <text x="590" y="75" text-anchor="middle" fill="#e2e8f0" font-size="12">LM Head</text>
            <rect x="670" y="40" width="140" height="60" rx="10" fill="#0b1224" stroke="#1f2942" />
            <text x="740" y="75" text-anchor="middle" fill="#e2e8f0" font-size="12">Softmax
(50k vocab)</text>
            <path d="M120 70h18" stroke="url(#g1)" stroke-width="3"/>
            <path d="M380 70h18" stroke="url(#g1)" stroke-width="3"/>
            <path d="M510 70h18" stroke="url(#g1)" stroke-width="3"/>
            <path d="M650 70h18" stroke="url(#g1)" stroke-width="3"/>
          </svg>
          <p class="lead" style="margin-top:8px">Context length 192 · d=384 · 6 heads · FFN=1536 · vocab≈50k</p>
        </div>
      </div>

      <details style="margin-top:12px">
        <summary><strong>Hyperparameters</strong></summary>
        <div class="code" style="margin-top:8px"><pre>optimizer: AdamW(lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-8)
schedule: warmup 100 steps → cosine to 3e-5 by step 5,000
batch: 24    context: 192    steps: 5,000
seed: 7      device: T4 (Colab)</pre></div>
      </details>
    </div>
  </section>

  <section class="section" id="samples">
    <div class="container">
      <h2>Sample generations</h2>
      <p class="lead">Temperature 0.8, top‑k 40. These are single‑pass samples from the trained checkpoint.</p>
      <p><em>Prompt</em>: <code>Once upon a time, </code></p>
      <blockquote>
        Once upon a time, there was a little girl named Lily. She loved to play with her toys all day long. One day, she found a special toy with a big rock. She was very excited to go on a trip to the park.
        
        Lily asked her mom, "Why is the same thing in the kitchen?"
        
        Lily replied, "No, it's a new boat. It's a nice car that looks like a toy. It's very cool for you."
        
        Lily looked at the book and saw the ball. She looked at it and said, "Lily,
      </blockquote>
      <p class="muted">Add more short samples here after you generate locally.</p>
    </div>
  </section>

  <section class="section" id="reproduce">
    <div class="container">
      <h2>Reproduce this run</h2>
      <div class="code"><pre># 1) Prepare TinyStories (HF → memmap, GPT‑2 BPE, one &lt;|endoftext|&gt; per story)
python tools/prepare_hf.py \
  dataset_name=roneneldan/TinyStories text_field=text \
  out_dir=data/Prepared/TinyStoriesHF append_eot=true dtype=uint16 tokenizer_name=gpt2

# 2) Train (~50–55 minutes on T4)
python -m sprintlm.train.train \
  dataset.train_bin=data/Prepared/TinyStoriesHF/train.bin \
  dataset.val_bin=data/Prepared/TinyStoriesHF/val.bin \
  train.batch_size=24 dataset.context_length=192 \
  train.max_steps=5000 train.log_every=50 train.eval_every=250 train.save_every=250

# 3) Plot and sample
python tools/plot_metrics.py outputs
python tools/decode_cli.py --ckpt outputs/&lt;RUN&gt;/ckpts/step5000.pt \
  --prompt "Once upon a time, " --context_length 192 --temperature 0.8 --top_k 40</pre></div>
    </div>
  </section>

  <footer>
    <div class="container">
      <div>© <span id="y"></span> SprintLM. Built with Python + PyTorch. Design: calm, readable, single‑file site.</div>
    </div>
  </footer>
  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
</body>
</html>