<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="color-scheme" content="light" />
  <title>SprintLM — a tiny GPT you can read</title>
  <meta name="description" content="SprintLM: a tiny GPT-style Transformer implemented from scratch and trained on TinyStories in ~1 hour." />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&display=swap" rel="stylesheet">
  <style>
    /* ——— Base ——— */
    :root{ --ink:#000; --paper:#fff; --line:#e5e5e5; --dim:#666; --nav-sep:#333; }
    *{ box-sizing:border-box }
    html,body{ height:100% }
    body{
      -webkit-font-smoothing:antialiased; -moz-osx-font-smoothing:grayscale; text-rendering:optimizeLegibility;
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, "SF Mono", Menlo, Consolas, "Liberation Mono", monospace;
      font-size:14px; line-height:1.7; letter-spacing:.01em;
      color:var(--ink); background:var(--paper);
      max-width:720px; margin:0 auto; padding:64px 24px 72px;
    }
    ::selection{ background:#f2f2f2 }

    /* ——— Type ——— */
    h1{ font-size:32px; line-height:1.15; margin:0 0 8px 0; font-weight:600 }
    h2{ font-size:22px; line-height:1.25; margin:40px 0 12px; font-weight:600; letter-spacing:.01em }
    h3{ font-size:15px; margin:16px 0 8px; font-weight:600; letter-spacing:.01em }
    p{ margin:0 0 14px }
    .sub{ margin-bottom:32px; padding-bottom:24px; border-bottom:1px solid var(--line) }
    a{ color:var(--ink); text-underline-offset:3px }
    a:hover{ opacity:.7 }
    hr{ border:none; border-top:1px solid var(--line); margin:36px 0 }

    /* ——— Darker, more aligned nav ——— */
    .nav{ 
      display:flex; 
      flex-wrap:wrap; 
      gap:12px; 
      margin:0 0 32px;
      padding:16px 0;
      border-top:1px solid var(--line);
      border-bottom:1px solid var(--line);
      justify-content:center;
    }
    .nav a{ text-decoration:none; font-weight:500 }
    .nav span{ 
      opacity:.8; 
      color:var(--nav-sep);
      font-weight:500;
    }

    /* ——— Better section spacing ——— */
    section{ margin-bottom:48px }
    section:last-of-type{ margin-bottom:0 }
    
    #overview{ 
      margin-top:24px;
      margin-bottom:48px;
    }

    /* ——— Definition grids (spec sheets) ——— */
    dl{ display:grid; grid-template-columns: 140px 1fr; gap:6px 14px; margin:10px 0 }
    dt{ color:var(--ink); font-weight:500 }
    dd{ margin:0 }

    /* ——— Figures ——— */
    figure{ margin:14px 0 18px }
    figure img{ width:100%; display:block }
    figcaption{ color:var(--dim); font-size:12px; margin-top:6px }

    /* ——— Sample ——— */
    .sample{ border-left:2px solid #ddd; padding:10px 14px; margin:14px 0; font-style:italic }
    .meta{ margin-top:8px; font-style:normal }

    pre{ background:#fafafa; border:1px solid var(--line); border-radius:4px; padding:12px; overflow:auto; margin:8px 0 }
    code{ font-family:inherit }

    footer{ border-top:1px solid var(--line); margin-top:48px; padding-top:16px }

    @media (max-width:720px){ 
      body{ padding:48px 16px 56px } 
      dl{ grid-template-columns:120px 1fr }
      .nav{ justify-content:flex-start; gap:8px }
    }
  </style>
</head>
<body>
  <header>
    <h1>SprintLM</h1>
    <p class="sub">A tiny GPT you can read end‑to‑end.</p>
    
    <nav class="nav">
      <a href="#overview">overview</a><span>·</span>
      <a href="#architecture">architecture</a><span>·</span>
      <a href="#training">training</a><span>·</span>
      <a href="#results">results</a><span>·</span>
      <a href="#samples">samples</a><span>·</span>
      <a href="#reproduce">reproduce</a><span>·</span>
      <a href="#limits">limits & next</a>
    </nav>

    <section id="overview">
      <p>From‑scratch GPT‑style Transformer trained on TinyStories in ~1 hour on a single T4. Decoder‑only with RoPE, pre‑RMSNorm, and SwiGLU. Code favors clarity over cleverness; the Python path intentionally omits a KV‑cache for readability.</p>
      <p>Source on <a href="https://github.com/leylinTheNinth/sprintlm.git" target="_blank" rel="noopener">GitHub</a>.</p>
      <p><em>Run headline:</em> val loss <strong>2.713</strong> → ppl <strong>15.07</strong>; tokens <strong>23.04M</strong>; throughput ~<strong>7.5k tok/s</strong>; 12 layers · d384 · 6 heads.</p>
    </section>
  </header>

  <section id="architecture">
    <h2>Architecture</h2>
    <dl>
      <dt>vocab</dt><dd>≈50k (GPT‑2 BPE via tiktoken)</dd>
      <dt>d_model</dt><dd>384</dd>
      <dt>layers</dt><dd>12</dd>
      <dt>heads</dt><dd>6</dd>
      <dt>d_ff</dt><dd>1536 (SwiGLU)</dd>
      <dt>context</dt><dd>192 tokens</dd>
      <dt>pos enc</dt><dd>RoPE (θ=10000)</dd>
      <dt>norm</dt><dd>RMSNorm (pre‑norm)</dd>
      <dt>attn</dt><dd>masked MHA, causal</dd>
    </dl>
  </section>

  <section id="training">
    <h2>Training</h2>
    <dl>
      <dt>optimizer</dt><dd>AdamW</dd>
      <dt>lr</dt><dd>3e‑4</dd>
      <dt>betas</dt><dd>(0.9, 0.95)</dd>
      <dt>weight decay</dt><dd>0.1</dd>
      <dt>eps</dt><dd>1e‑8</dd>
      <dt>schedule</dt><dd>warmup 100 → cosine</dd>
      <dt>batch</dt><dd>24</dd>
      <dt>steps</dt><dd>5000</dd>
      <dt>seed</dt><dd>7</dd>
      <dt>device</dt><dd>1× T4 (free Colab)</dd>
    </dl>
  </section>

  <section id="results">
    <h2>Results</h2>
    <figure>
      <img src="assets/loss_train.png" alt="Training loss curve" loading="lazy" />
      <figcaption>Training loss over 5,000 steps (batch 24, context 192).</figcaption>
    </figure>
    <figure>
      <img src="assets/loss_val.png" alt="Validation loss curve" loading="lazy" />
      <figcaption>Validation loss on TinyStories. Final: 2.713 → ppl 15.07.</figcaption>
    </figure>
    <figure>
      <img src="assets/tokps.png" alt="Tokens per second across training" loading="lazy" />
      <figcaption>Throughput on a T4. Dips correspond to eval/ckpt saves.</figcaption>
    </figure>
  </section>

  <section id="samples">
    <h2>Sample</h2>
    <p>Generated with temperature 0.8, top‑k 40:</p>
    <div class="sample">
      Once upon a time, there was a little girl named Lily. She loved to play with her toys all day long. One day, she found a special toy with a big rock. She was very excited to go on a trip to the park.
      
      Lily asked her mom, "Why is the same thing in the kitchen?"
      
      Lily replied, "No, it's a new boat. It's a nice car that looks like a toy. It's very cool for you."
      
      Lily looked at the book and saw the ball. She looked at it and said, "Lily,
      <div class="meta">prompt: "Once upon a time, "</div>
    </div>
  </section>

  <section id="reproduce">
    <h2>Reproduce</h2>

    <h3>Prepare dataset</h3>
    <pre>python tools/prepare_hf.py \
  dataset_name=roneneldan/TinyStories text_field=text \
  out_dir=data/Prepared/TinyStoriesHF append_eot=true \
  dtype=uint16 tokenizer_name=gpt2</pre>

    <h3>Train model</h3>
    <pre>python -m sprintlm.train.train \
  dataset.train_bin=data/Prepared/TinyStoriesHF/train.bin \
  dataset.val_bin=data/Prepared/TinyStoriesHF/val.bin \
  train.batch_size=24 dataset.context_length=192 \
  train.max_steps=5000 train.log_every=50 \
  train.eval_every=250 train.save_every=250</pre>

    <h3>Generate & visualize</h3>
    <pre>python tools/plot_metrics.py outputs
python tools/decode_cli.py --ckpt outputs/&lt;RUN&gt;/ckpts/step5000.pt \
  --prompt "Once upon a time, " --context_length 192 \
  --temperature 0.8 --top_k 40</pre>

    <p>Platform notes: M4 MacBook (16 GB) use batch_size≈8‑12 with MPS. A100/H100 can scale to batch_size≥64 and context≥512.</p>
  </section>

  <section id="limits">
    <h2>Limits & next</h2>
    <p><strong>Limits.</strong> TinyStories domain (child‑story style); not instruction‑tuned; Python decoder is intentionally minimal (no KV‑cache); GPT‑2 BPE only; no dataset filtering or dedup.</p>
    <p><strong>Next.</strong> Minimal C++17 CPU inference (weights export → single‑file greedy decoder); optional safetensors release; KV‑cache in Python for longer generations; basic quantization (int8/gguf) and latency measurements on M4 CPU; Colab quickstart notebook; model card on HF.</p>
  </section>

  <footer>
    <p>Clean implementation designed for learning and experimentation. © <span id="y"></span> SprintLM.</p>
  </footer>
  <script>document.getElementById('y').textContent=new Date().getFullYear();</script>
</body>
</html>